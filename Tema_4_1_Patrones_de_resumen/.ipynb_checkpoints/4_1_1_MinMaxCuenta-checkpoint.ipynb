{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memoria TFG. Tema 4. Apartado 1. Patrones de resumen. Resúmenes numéricos. Valor mínimo, máximo y contar valores Versión 1\n",
    "\n",
    "Patrón para extraer el valor máximo y el valor mínimo de un conjunto de valores. En este ejemplo usamos un tabla con clientes que han comporado en distintas fechas; filtrará  el valor más alto y más bajo de un cliente y el imoprte total de número de compras realizadas. Este patrón podríamos englobarlo dentro de los patrones de filtrado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting minmaxcount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile minmaxcount.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class minmaxcount(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "       \n",
    "        linea=line.split(\";\")\n",
    "        valor=linea[0] \n",
    "        clave=linea[1] \n",
    "        \n",
    "        yield clave,float(valor)\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "        #Lista donde almacenaremos los valores\n",
    "        valores=[]\n",
    "        \n",
    "        #Recorremos la lista de \"values\"\n",
    "        #para agregarlos a la lista\n",
    "        for value in values:\n",
    "            valores.append(value)\n",
    "        \n",
    "        #Ordenamos los valores de la lista\n",
    "        valores.sort()\n",
    "        #Valor máximo\n",
    "        vMax=valores[len(valores)-1] \n",
    "        #Valor mínimo\n",
    "        vMin=valores[0]\n",
    "        #Número de valores del grupo\n",
    "        cuenta=len(valores)\n",
    "        \n",
    "        yield key, (cuenta,vMax,vMin)\n",
    "                \n",
    "if __name__ == '__main__':\n",
    "    minmaxcount.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory C:\\Users\\MRSANC~1\\AppData\\Local\\Temp\\minmaxcount.mrsanchez.20201010.173520.775940\n",
      "Running step 1 of 1...\n",
      "job output is in C:\\Users\\MRSANC~1\\AppData\\Local\\Temp\\minmaxcount.mrsanchez.20201010.173520.775940\\output\n",
      "Streaming final output from C:\\Users\\MRSANC~1\\AppData\\Local\\Temp\\minmaxcount.mrsanchez.20201010.173520.775940\\output...\n",
      "Removing temp directory C:\\Users\\MRSANC~1\\AppData\\Local\\Temp\\minmaxcount.mrsanchez.20201010.173520.775940...\n"
     ]
    }
   ],
   "source": [
    "!python minmaxcount.py Venta-cliente.csv > mixmaxcount.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memoria TFG. Tema 4. Apartado 1. Patrones de resumen. Resúmenes numéricos. Valor mínimo, máximo y contar valores Versión 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting minmaxcountV2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile minmaxcountV2.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import statistics as st\n",
    "\n",
    "class minmaxcountV2(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "       \n",
    "        linea=line.split(\";\")\n",
    "        valor=linea[0] \n",
    "        clave=linea[1] \n",
    "        \n",
    "        yield clave,float(valor)\n",
    "    \n",
    "    def reducer(self, key, values):\n",
    "        valores=list(values)\n",
    "        \n",
    "        valores.sort()\n",
    "        vMax=valores[len(valores)-1]\n",
    "        vMin=valores[0]\n",
    "        cuenta=len(valores)\n",
    "        \n",
    "        yield key, (cuenta,vMax,vMin)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    minmaxcountV2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory C:\\Users\\MRSANC~1\\AppData\\Local\\Temp\\minmaxcountV2.mrsanchez.20201010.175527.042940\n",
      "Running step 1 of 1...\n",
      "job output is in C:\\Users\\MRSANC~1\\AppData\\Local\\Temp\\minmaxcountV2.mrsanchez.20201010.175527.042940\\output\n",
      "Streaming final output from C:\\Users\\MRSANC~1\\AppData\\Local\\Temp\\minmaxcountV2.mrsanchez.20201010.175527.042940\\output...\n",
      "Removing temp directory C:\\Users\\MRSANC~1\\AppData\\Local\\Temp\\minmaxcountV2.mrsanchez.20201010.175527.042940...\n"
     ]
    }
   ],
   "source": [
    "!python minmaxcountV2.py Venta-cliente.csv > mixmaxcountV2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/lib/hadoop/bin...\n",
      "Found hadoop binary: /usr/lib/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/minmaxcount.root.20200215.165639.320876\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/minmaxcount.root.20200215.165639.320876/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/minmaxcount.root.20200215.165639.320876/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob3552117603961381211.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1581762628186_0002\n",
      "  Submitted application application_1581762628186_0002\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1581762628186_0002/\n",
      "  Running job: job_1581762628186_0002\n",
      "  Job job_1581762628186_0002 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1581762628186_0002 completed successfully\n",
      "  Output directory: hdfs:///tmp/salidaMinmaxcount\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=282778\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=27877\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=399234\n",
      "\t\tFILE: Number of bytes written=1243298\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=282966\n",
      "\t\tHDFS: Number of bytes written=27877\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=42513408\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=15312896\n",
      "\t\tTotal time spent by all map tasks (ms)=41517\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=41517\n",
      "\t\tTotal time spent by all reduce tasks (ms)=14954\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=14954\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=41517\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=14954\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=8780\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=943\n",
      "\t\tInput split bytes=188\n",
      "\t\tMap input records=29999\n",
      "\t\tMap output bytes=339230\n",
      "\t\tMap output materialized bytes=399240\n",
      "\t\tMap output records=29999\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=786485248\n",
      "\t\tReduce input groups=756\n",
      "\t\tReduce input records=29999\n",
      "\t\tReduce output records=2268\n",
      "\t\tReduce shuffle bytes=399240\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=59998\n",
      "\t\tTotal committed heap usage (bytes)=649068544\n",
      "\t\tVirtual memory (bytes) snapshot=7883583488\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///tmp/salidaMinmaxcount\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/minmaxcount.root.20200215.165639.320876...\n",
      "Removing temp directory /tmp/minmaxcount.root.20200215.165639.320876...\n"
     ]
    }
   ],
   "source": [
    "! python minmaxcount.py hdfs:///tmp/Venta-cliente.csv -r hadoop --python-bin /opt/anaconda/bin/python3.7 --output-dir hdfs:///tmp/salidaMinmaxcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
