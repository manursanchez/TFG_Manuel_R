{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import numpy as np\n",
    "import mrjob\n",
    "\n",
    "class MRKmeans(MRJob):\n",
    "\n",
    "    OUTPUT_PROTOCOL = mrjob.protocol.RawProtocol\n",
    "    ndim=None\n",
    "    Centroid=None\n",
    "    nclass=None\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MRKmeans, self).__init__(*args, **kwargs)\n",
    "        global _global_dict\n",
    "        _global_dict = {}\n",
    "\n",
    "    \n",
    "\n",
    "    def configure_args(self):\n",
    "        \n",
    "        super(MRKmeans, self).configure_args()\n",
    "        self.add_file_arg('--centroids_input')\n",
    "        self.add_file_arg('--centroids_output')    \n",
    "        self.add_passthru_arg('--iterations', help='iterations', default=10, type=int)        \n",
    "\n",
    "    def load_args(self,args):\n",
    "       \n",
    "        super(MRKmeans,self).load_args(args)\n",
    "        \n",
    "        if self.options.centroids_input is None:\n",
    "            self.option_parser.error(\"please type the centroids input file.\")\n",
    "        else:\n",
    "            self.infile = self.options.centroids_input\n",
    "        \n",
    "        if self.options.centroids_output is None:\n",
    "            self.outfile = self.infile\n",
    "        else:\n",
    "            self.outfile = self.options.centroids_output\n",
    "        self.iterations = self.options.iterations\n",
    "\n",
    "    def get_centroids(self):\n",
    "        \n",
    "        Centroid = np.loadtxt(self.infile, delimiter = ',')\n",
    "        return Centroid\n",
    "\n",
    "    def write_centroids(self, Centroid):\n",
    "       \n",
    "        np.savetxt(\"./foo.txt\", Centroid[None], fmt = '%.5f',delimiter = ',')\n",
    " \n",
    "    def relabel_data(self, _, line):\n",
    "        '''\n",
    "        Mapper\n",
    "        '''\n",
    "        try:\n",
    "            Coord, Cluster_ID = line.split('|')\n",
    "        except:\n",
    "            Coord = line\n",
    "        Coord_arr = np.array(Coord.split(','), dtype = float)\n",
    "        global Centroid\n",
    "        Centroid = self.get_centroids()\n",
    "        Centroid_arr = np.reshape(Centroid, (-1, len(Coord_arr)))\n",
    "        global nclass\n",
    "        global ndim\n",
    "        nclass = Centroid_arr.shape[0]\n",
    "        ndim = Centroid_arr.shape[1]\n",
    "        \n",
    "        Distance = ((Centroid_arr - Coord_arr)**2).sum(axis = 1)\n",
    "        Cluster_ID = str(Distance.argmin() + 1)\n",
    "        Coord_arr = Coord_arr.tolist()\n",
    "        yield Cluster_ID, Coord_arr\n",
    "    \n",
    "    def node_combine(self, Cluster_ID, values):\n",
    "        '''\n",
    "        Combiner\n",
    "        '''\n",
    "        \n",
    "        global ndim\n",
    "        Coord_set = []\n",
    "        Coord_sum = np.zeros(ndim)\n",
    "        for Coord_arr in values:\n",
    "            Coord_set.append(','.join(str(e) for e in Coord_arr))\n",
    "            Coord_arr = np.array(Coord_arr, dtype = float)\n",
    "            Coord_sum += Coord_arr\n",
    "            Coord_sum = Coord_sum.tolist()\n",
    "        yield Cluster_ID, (Coord_sum, Coord_set)\n",
    "    \n",
    "    def update_centroid(self, Cluster_ID, values):\n",
    "        '''\n",
    "        Reducer\n",
    "        '''\n",
    "        global ndim\n",
    "        global Centroid\n",
    "        global nclass\n",
    "        final_Coord_set = []\n",
    "        n = 0\n",
    "        final_Coord_sum = np.zeros(ndim)\n",
    "        for Coord_sum, Coord_set in values:\n",
    "            final_Coord_set += Coord_set\n",
    "            Coord_sum = np.array(Coord_sum, dtype = float)\n",
    "            final_Coord_sum += Coord_sum\n",
    "            n += 1\n",
    "        \n",
    "        new_Centroid = final_Coord_sum / n\n",
    "        Centroid[ndim * (int(Cluster_ID) - 1) : ndim * int(Cluster_ID)] = new_Centroid\n",
    "        if int(Cluster_ID) == nclass:\n",
    "\t        self.write_centroids(Centroid)\n",
    "\n",
    "        for final_Coord in final_Coord_set:\n",
    "            yield None, (final_Coord + '|' + Cluster_ID)\n",
    "\n",
    "         \n",
    "    def steps(self):   \n",
    "        return [MRStep(mapper=self.relabel_data,\n",
    "                       combiner=self.node_combine,\n",
    "                       reducer=self.update_centroid)] * self.iterations \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRKmeans.run() \n",
    "python ./code/KMeans.py -r local \\\n",
    "--centroids_input ./test_dataset/Centroid.txt \\\n",
    "--centroids_output ./test_dataset/Centroid.txt  \\\n",
    "--iterations 10 \\\n",
    "./test_dataset/e.txt \n",
    "Probable cause of failure:\n",
    "\n",
    "+ /home/logic/anaconda3/bin/python KMeans.py --step-num=0 --combiner --centroids_input Centroid.txt --centroids_output Centroid-1.txt --iterations 10\n",
    "Traceback (most recent call last):\n",
    "  File \"KMeans.py\", line 132, in <module>\n",
    "    MRKmeans.run() \n",
    "  File \"/tmp/KMeans.logic.20200423.102930.879303/step/000/combiner/00000/wd/mrjob.zip/mrjob/job.py\", line 616, in run\n",
    "  File \"/tmp/KMeans.logic.20200423.102930.879303/step/000/combiner/00000/wd/mrjob.zip/mrjob/job.py\", line 678, in execute\n",
    "  File \"/tmp/KMeans.logic.20200423.102930.879303/step/000/combiner/00000/wd/mrjob.zip/mrjob/job.py\", line 780, in run_combiner\n",
    "  File \"/tmp/KMeans.logic.20200423.102930.879303/step/000/combiner/00000/wd/mrjob.zip/mrjob/job.py\", line 850, in combine_pairs\n",
    "  File \"/tmp/KMeans.logic.20200423.102930.879303/step/000/combiner/00000/wd/mrjob.zip/mrjob/job.py\", line 889, in _combine_or_reduce_pairs\n",
    "  File \"KMeans.py\", line 92, in node_combine\n",
    "    Coord_sum = np.zeros(ndim)\n",
    "NameError: name 'ndim' is not defined\n",
    "\n",
    "(from lines 7-18 of /tmp/KMeans.logic.20200423.102930.879303/step/000/combiner/00000/stderr)\n",
    "\n",
    "while reading input from /tmp/KMeans.logic.20200423.102930.879303/step/000/combiner/00000/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have the following structure MRJOb:\n",
    "\n",
    "def mapper1(self, _, line):\n",
    "...\n",
    "\n",
    "def reducer1(self, nodeA, nodeP):\n",
    "...\n",
    "\n",
    "def mapper2(self, nodeA, nodeP):\n",
    "...\n",
    "\n",
    "def steps(self):\n",
    "        return [MRStep(mapper=self.mapper1, reducer=self.reducer1),\n",
    "                MRStep(mapper=self.mapper2)]\n",
    "I execute the program with the following command:\n",
    "\n",
    "python main.py a.txt\n",
    "\n",
    "I want iterate for N times, the steps of MRJob with the same input file. How to solve? Thank you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_args(self):\n",
    "    super(MRYourJob, self).configure_args()\n",
    "    self.add_passthru_arg(\n",
    "            '-n', '--num-iterations', default=1, type=int,\n",
    "            help='Number of times to run the job')\n",
    "\n",
    "def steps(self):\n",
    "    return [MRStep(mapper=self.mapper1, reducer=self.reducer1),\n",
    "                MRStep(mapper=self.mapper2)] * self.options.num_iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to join between two files,but I get Error error : NameError: name 'names' is not defined\n",
    "\n",
    "!python job.py data.txt --database item.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "class MRPeopleScores(MRJob):\n",
    "     def steps(self):\n",
    "            return [\n",
    "             MRStep(mapper=self.mapper_1,reducer_init=self.reducer_init, reducer=self.reducer_1)\n",
    "         ]\n",
    "   \n",
    "    def configure_args(self):\n",
    "        super(MRPeopleScores, self).configure_args()\n",
    "        self.add_file_arg('--database')\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        (employee_id, age,var_,salary) = line.split(\"\\t\")\n",
    "        yield int(employee_id), salary\n",
    "        \n",
    "    def reducer_init(self):\n",
    "        \n",
    "        with open(\"item.txt\") as f:\n",
    "            for line in f:\n",
    "                fields = line.split('|')\n",
    "                self.names[fields[0]] = fields[1] \n",
    "\n",
    "    def reducer(self,employee_id, salary):\n",
    "        \n",
    "        for salary_ in salary:\n",
    "              yield employee_id,(salary_,names[employee_id])\n",
    "   \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRPeopleScores.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
