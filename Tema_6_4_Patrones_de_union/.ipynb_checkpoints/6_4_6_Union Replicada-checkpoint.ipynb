{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capítulo 6\n",
    "\n",
    "6.4 Patrones de unión.\n",
    "\n",
    "6.4.6 Unión replicada \n",
    "\n",
    "Prototipo en el que cargamos la primera tabla en memoria, y posteriomente metemos por línea de comandos todos las tablas que queramos unir a esa primera tabla. Las uniones que se producen pueden ser internas y/o externas por la izquierda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting unionReplicada.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile unionReplicada.py\n",
    "#!/usr/bin/python\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "\n",
    "def cargarFicheroEnMemoria(self,fichero):\n",
    "    diccionario={}\n",
    "    with open(fichero) as f:\n",
    "        self.tablaEnMemoria = set(line.strip() for line in f)\n",
    "    for linea in self.tablaEnMemoria:\n",
    "        #Para que no tenga en cuenta las cabeceras de las tablas\n",
    "        encontrado=re.search('[a-zA-Z]',linea[0])\n",
    "        if encontrado==None:\n",
    "            datos = linea.split(\";\")\n",
    "            diccionario[datos[0]]=datos\n",
    "    return diccionario\n",
    "\n",
    "class unionReplicada(MRJob):\n",
    "   \n",
    "    #FILES = ['hdfs:///archivos_datos/tiendas.csv'] #Para ejecución en el CLUSTER\n",
    "    FILES = ['archivos_datos/tiendas-articulos/tiendas.csv'] # Para ejecución en local\n",
    "    fichero='tiendas.csv'    \n",
    "    def mapper_init(self):\n",
    "        #Nos devuelve la estructura diccionario rellena con los datos del fichero\n",
    "        self.dicTablaA=cargarFicheroEnMemoria(self,self.fichero) \n",
    "       \n",
    "    def mapper(self,_,line):\n",
    "        self.linea=line.split(';')\n",
    "        encontrado=re.search('[a-zA-Z]',self.linea[0])\n",
    "        if encontrado==None:\n",
    "            self.clave=self.linea[1] #Compara Clave de la tabla/s del stream \n",
    "            if self.clave in self.dicTablaA: \n",
    "                yield self.clave,(self.linea,self.dicTablaA.get(self.clave) )\n",
    "            else:\n",
    "                yield self.clave,(self.linea,\"null\")\n",
    "                \n",
    "if __name__ == '__main__':\n",
    "    unionReplicada.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILES: archivos_datos/tiendas-articulos/tiendas.csv will appear in same directory as job script, not a subdirectory\n",
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/unionReplicada.root.20201214.230312.067818\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/unionReplicada.root.20201214.230312.067818/output\n",
      "Streaming final output from /tmp/unionReplicada.root.20201214.230312.067818/output...\n",
      "\"3\"\t[[\"1028\", \"3\", \"61\"], [\"3\", \"Granada\"]]\n",
      "\"4\"\t[[\"1029\", \"4\", \"2\"], \"null\"]\n",
      "\"5\"\t[[\"1030\", \"5\", \"67\"], \"null\"]\n",
      "\"3\"\t[[\"1011\", \"3\", \"16\"], [\"3\", \"Granada\"]]\n",
      "\"5\"\t[[\"1012\", \"5\", \"2\"], \"null\"]\n",
      "\"2\"\t[[\"1013\", \"2\", \"17\"], [\"2\", \"Albacete\"]]\n",
      "\"1\"\t[[\"1014\", \"1\", \"9\"], [\"1\", \"Santander\"]]\n",
      "\"2\"\t[[\"1015\", \"2\", \"7\"], [\"2\", \"Albacete\"]]\n",
      "\"3\"\t[[\"1016\", \"3\", \"2\"], [\"3\", \"Granada\"]]\n",
      "\"5\"\t[[\"1017\", \"5\", \"1\"], \"null\"]\n",
      "\"2\"\t[[\"1018\", \"2\", \"9\"], [\"2\", \"Albacete\"]]\n",
      "\"1\"\t[[\"1019\", \"1\", \"13\"], [\"1\", \"Santander\"]]\n",
      "\"1\"\t[[\"1020\", \"1\", \"23\"], [\"1\", \"Santander\"]]\n",
      "\"5\"\t[[\"1021\", \"5\", \"25\"], \"null\"]\n",
      "\"4\"\t[[\"1022\", \"4\", \"100\"], \"null\"]\n",
      "\"2\"\t[[\"1023\", \"2\", \"20\"], [\"2\", \"Albacete\"]]\n",
      "\"1\"\t[[\"1024\", \"1\", \"12\"], [\"1\", \"Santander\"]]\n",
      "\"3\"\t[[\"1025\", \"3\", \"6\"], [\"3\", \"Granada\"]]\n",
      "\"5\"\t[[\"1026\", \"5\", \"2\"], \"null\"]\n",
      "\"1\"\t[[\"1027\", \"1\", \"3\"], [\"1\", \"Santander\"]]\n",
      "\"2\"\t[[\"1008\", \"2\", \"5\"], [\"2\", \"Albacete\"]]\n",
      "\"4\"\t[[\"1009\", \"4\", \"1\"], \"null\"]\n",
      "\"3\"\t[[\"1010\", \"3\", \"20\"], [\"3\", \"Granada\"]]\n",
      "\"1\"\t[[\"1001\", \"1\", \"20\"], [\"1\", \"Santander\"]]\n",
      "\"2\"\t[[\"1002\", \"2\", \"10\"], [\"2\", \"Albacete\"]]\n",
      "\"1\"\t[[\"1003\", \"1\", \"15\"], [\"1\", \"Santander\"]]\n",
      "\"5\"\t[[\"1004\", \"5\", \"10\"], \"null\"]\n",
      "\"2\"\t[[\"1005\", \"2\", \"5\"], [\"2\", \"Albacete\"]]\n",
      "\"3\"\t[[\"1006\", \"3\", \"3\"], [\"3\", \"Granada\"]]\n",
      "\"1\"\t[[\"1007\", \"1\", \"8\"], [\"1\", \"Santander\"]]\n",
      "Removing temp directory /tmp/unionReplicada.root.20201214.230312.067818...\n"
     ]
    }
   ],
   "source": [
    "!python unionReplicada.py archivos_datos/tiendas-articulos/articulos_stock.csv archivos_datos/tiendas-articulos/articulos_stock2.csv archivos_datos/tiendas-articulos/articulos_stock3.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EJECUCIÓN EN EL CLUSTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILES: hdfs:///archivos_datos/tiendas.csv will appear in same directory as job script, not a subdirectory\n",
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/lib/hadoop/bin...\n",
      "Found hadoop binary: /usr/lib/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/unionReplicada.root.20201214.231725.899226\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/unionReplicada.root.20201214.231725.899226/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/unionReplicada.root.20201214.231725.899226/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob6752507925837337719.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.5:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.5:8032\n",
      "  Total input paths to process : 3\n",
      "  Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1249)\n",
      "\tat java.lang.Thread.join(Thread.java:1323)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:969)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeInternal(DFSOutputStream.java:937)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:933)\n",
      "  number of splits:3\n",
      "  Submitting tokens for job: job_1607985144237_0006\n",
      "  Submitted application application_1607985144237_0006\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1607985144237_0006/\n",
      "  Running job: job_1607985144237_0006\n",
      "  Job job_1607985144237_0006 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1607985144237_0006 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/unionReplicada.root.20201214.231725.899226/output\n",
      "Counters: 31\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=382\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1239\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=444816\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=705\n",
      "\t\tHDFS: Number of bytes written=1239\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=117856256\n",
      "\t\tTotal time spent by all map tasks (ms)=115094\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=115094\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=115094\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=7570\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=2740\n",
      "\t\tInput split bytes=323\n",
      "\t\tMap input records=33\n",
      "\t\tMap output records=30\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=603086848\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=532676608\n",
      "\t\tVirtual memory (bytes) snapshot=7890280448\n",
      "job output is in hdfs:///user/root/tmp/mrjob/unionReplicada.root.20201214.231725.899226/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/unionReplicada.root.20201214.231725.899226/output...\n",
      "\"5\"\t[[\"1021\", \"5\", \"25\"], \"null\"]\n",
      "\"4\"\t[[\"1022\", \"4\", \"100\"], \"null\"]\n",
      "\"2\"\t[[\"1023\", \"2\", \"20\"], [\"2\", \"Albacete\"]]\n",
      "\"1\"\t[[\"1024\", \"1\", \"12\"], [\"1\", \"Santander\"]]\n",
      "\"3\"\t[[\"1025\", \"3\", \"6\"], [\"3\", \"Granada\"]]\n",
      "\"5\"\t[[\"1026\", \"5\", \"2\"], \"null\"]\n",
      "\"1\"\t[[\"1027\", \"1\", \"3\"], [\"1\", \"Santander\"]]\n",
      "\"3\"\t[[\"1028\", \"3\", \"61\"], [\"3\", \"Granada\"]]\n",
      "\"4\"\t[[\"1029\", \"4\", \"2\"], \"null\"]\n",
      "\"5\"\t[[\"1030\", \"5\", \"67\"], \"null\"]\n",
      "\"1\"\t[[\"1001\", \"1\", \"20\"], [\"1\", \"Santander\"]]\n",
      "\"2\"\t[[\"1002\", \"2\", \"10\"], [\"2\", \"Albacete\"]]\n",
      "\"1\"\t[[\"1003\", \"1\", \"15\"], [\"1\", \"Santander\"]]\n",
      "\"5\"\t[[\"1004\", \"5\", \"10\"], \"null\"]\n",
      "\"2\"\t[[\"1005\", \"2\", \"5\"], [\"2\", \"Albacete\"]]\n",
      "\"3\"\t[[\"1006\", \"3\", \"3\"], [\"3\", \"Granada\"]]\n",
      "\"1\"\t[[\"1007\", \"1\", \"8\"], [\"1\", \"Santander\"]]\n",
      "\"2\"\t[[\"1008\", \"2\", \"5\"], [\"2\", \"Albacete\"]]\n",
      "\"4\"\t[[\"1009\", \"4\", \"1\"], \"null\"]\n",
      "\"3\"\t[[\"1010\", \"3\", \"20\"], [\"3\", \"Granada\"]]\n",
      "\"3\"\t[[\"1011\", \"3\", \"16\"], [\"3\", \"Granada\"]]\n",
      "\"5\"\t[[\"1012\", \"5\", \"2\"], \"null\"]\n",
      "\"2\"\t[[\"1013\", \"2\", \"17\"], [\"2\", \"Albacete\"]]\n",
      "\"1\"\t[[\"1014\", \"1\", \"9\"], [\"1\", \"Santander\"]]\n",
      "\"2\"\t[[\"1015\", \"2\", \"7\"], [\"2\", \"Albacete\"]]\n",
      "\"3\"\t[[\"1016\", \"3\", \"2\"], [\"3\", \"Granada\"]]\n",
      "\"5\"\t[[\"1017\", \"5\", \"1\"], \"null\"]\n",
      "\"2\"\t[[\"1018\", \"2\", \"9\"], [\"2\", \"Albacete\"]]\n",
      "\"1\"\t[[\"1019\", \"1\", \"13\"], [\"1\", \"Santander\"]]\n",
      "\"1\"\t[[\"1020\", \"1\", \"23\"], [\"1\", \"Santander\"]]\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/unionReplicada.root.20201214.231725.899226...\n",
      "Removing temp directory /tmp/unionReplicada.root.20201214.231725.899226...\n"
     ]
    }
   ],
   "source": [
    "!python unionReplicada.py hdfs:///archivos_datos/articulos_stock.csv hdfs:///archivos_datos/articulos_stock2.csv hdfs:///archivos_datos/articulos_stock3.csv -r hadoop --python-bin /opt/anaconda/bin/python3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
