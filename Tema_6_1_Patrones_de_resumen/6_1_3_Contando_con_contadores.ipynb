{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capítulo 6\n",
    "\n",
    "6.1 Patrones de resumen.\n",
    "\n",
    "6.1.3. Contando con contadores Versión 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing counter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile counter.py\n",
    "#!/usr/bin/env python\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class counter(MRJob):\n",
    "    def mapper_init(self):\n",
    "        #Creamos un diccionario con los que serán los contadores a 0\n",
    "        self.contadores={\"Netherlands\":0,\"France\":0,\"Australia\":0}    \n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        linea=line.split(\";\")\n",
    "        # Estudiamos cada token de la línea recogido\n",
    "        for token in linea:\n",
    "            # Si el token está en el diccionario\n",
    "            if token in self.contadores:\n",
    "                # Contamos con el contador definido en diccionario\n",
    "                self.contadores[token]=self.contadores[token]+1\n",
    "          \n",
    "    def mapper_final(self):\n",
    "        yield \"Bloque: \",self.contadores\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    counter.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/counter.root.20201214.184253.110770\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/counter.root.20201214.184253.110770/output\n",
      "Streaming final output from /tmp/counter.root.20201214.184253.110770/output...\n",
      "\"Bloque: \"\t{\"Netherlands\": 593, \"France\": 2378, \"Australia\": 347}\n",
      "\"Bloque: \"\t{\"Netherlands\": 523, \"France\": 2151, \"Australia\": 45}\n",
      "\"Bloque: \"\t{\"Netherlands\": 694, \"France\": 2010, \"Australia\": 504}\n",
      "\"Bloque: \"\t{\"Netherlands\": 561, \"France\": 2018, \"Australia\": 363}\n",
      "Removing temp directory /tmp/counter.root.20201214.184253.110770...\n"
     ]
    }
   ],
   "source": [
    "!python counter.py archivos_datos/Online_Retail.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EJECUCIÓN EN EL CLUSTER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/lib/hadoop/bin...\n",
      "Found hadoop binary: /usr/lib/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/counter.root.20201214.184430.006726\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/counter.root.20201214.184430.006726/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/counter.root.20201214.184430.006726/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob4337350702395216145.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.5:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.5:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1607966861739_0009\n",
      "  Submitted application application_1607966861739_0009\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1607966861739_0009/\n",
      "  Running job: job_1607966861739_0009\n",
      "  Job job_1607966861739_0009 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1607966861739_0009 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/counter.root.20201214.184430.006726/output\n",
      "Counters: 30\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=45542142\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=134\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=295618\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=45542352\n",
      "\t\tHDFS: Number of bytes written=134\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=47475712\n",
      "\t\tTotal time spent by all map tasks (ms)=46363\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=46363\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=46363\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=7010\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=898\n",
      "\t\tInput split bytes=210\n",
      "\t\tMap input records=541910\n",
      "\t\tMap output records=2\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=414965760\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=361758720\n",
      "\t\tVirtual memory (bytes) snapshot=5267689472\n",
      "job output is in hdfs:///user/root/tmp/mrjob/counter.root.20201214.184430.006726/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/counter.root.20201214.184430.006726/output...\n",
      "\"Bloque: \"\t{\"Netherlands\": 1255, \"France\": 4028, \"Australia\": 867}\n",
      "\"Bloque: \"\t{\"Netherlands\": 1116, \"France\": 4529, \"Australia\": 392}\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/counter.root.20201214.184430.006726...\n",
      "Removing temp directory /tmp/counter.root.20201214.184430.006726...\n"
     ]
    }
   ],
   "source": [
    "! python counter.py hdfs:///archivos_datos/Online_Retail.csv -r hadoop --python-bin /opt/anaconda/bin/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capítulo 6\n",
    "\n",
    "6.1 Patrones de resumen.\n",
    "\n",
    "6.1.3. Contando con contadores Versión 2 donde incluimos el reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing counterV2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile counterV2.py\n",
    "#!/usr/bin/env python\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class counterV2(MRJob):\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        #Creamos un diccionario con los que serán los contadores a 0\n",
    "        self.contadores={\"Netherlands\":0,\"France\":0,\"Australia\":0}    \n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        linea=line.split(\";\")\n",
    "        # Estudiamos cada token de la línea recogido\n",
    "        for token in linea:\n",
    "            # Si el token está en el diccionario\n",
    "            if token in self.contadores:\n",
    "                # Contamos con el contador definido en diccionario\n",
    "                self.contadores[token]=self.contadores[token]+1\n",
    "          \n",
    "    def mapper_final(self):\n",
    "        for clave, valor in self.contadores.items(): \n",
    "            yield clave,valor\n",
    "   \n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    counterV2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/counterV2.root.20201214.184705.501517\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/counterV2.root.20201214.184705.501517/output\n",
      "Streaming final output from /tmp/counterV2.root.20201214.184705.501517/output...\n",
      "\"Netherlands\"\t2371\n",
      "\"France\"\t8557\n",
      "\"Australia\"\t1259\n",
      "Removing temp directory /tmp/counterV2.root.20201214.184705.501517...\n"
     ]
    }
   ],
   "source": [
    "!python counterV2.py archivos_datos/Online_Retail.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EJECUCIÓN EN EL CLUSTER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/lib/hadoop/bin...\n",
      "Found hadoop binary: /usr/lib/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/counterV2.root.20201214.184750.233206\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/counterV2.root.20201214.184750.233206/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/counterV2.root.20201214.184750.233206/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob8474357298020190119.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.5:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.5:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1607966861739_0010\n",
      "  Submitted application application_1607966861739_0010\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1607966861739_0010/\n",
      "  Running job: job_1607966861739_0010\n",
      "  Job job_1607966861739_0010 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1607966861739_0010 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/counterV2.root.20201214.184750.233206/output\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=45542142\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=116\n",
      "\t\tFILE: Number of bytes written=445128\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=45542352\n",
      "\t\tHDFS: Number of bytes written=50\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=44485632\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=10981376\n",
      "\t\tTotal time spent by all map tasks (ms)=43443\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=43443\n",
      "\t\tTotal time spent by all reduce tasks (ms)=10724\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=10724\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=43443\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=10724\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=7930\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=906\n",
      "\t\tInput split bytes=210\n",
      "\t\tMap input records=541910\n",
      "\t\tMap output bytes=98\n",
      "\t\tMap output materialized bytes=122\n",
      "\t\tMap output records=6\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=790908928\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=6\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=122\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=12\n",
      "\t\tTotal committed heap usage (bytes)=707788800\n",
      "\t\tVirtual memory (bytes) snapshot=7890800640\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/counterV2.root.20201214.184750.233206/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/counterV2.root.20201214.184750.233206/output...\n",
      "\"Australia\"\t1259\n",
      "\"France\"\t8557\n",
      "\"Netherlands\"\t2371\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/counterV2.root.20201214.184750.233206...\n",
      "Removing temp directory /tmp/counterV2.root.20201214.184750.233206...\n"
     ]
    }
   ],
   "source": [
    "! python counterV2.py hdfs:///archivos_datos/Online_Retail.csv -r hadoop --python-bin /opt/anaconda/bin/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capítulo 6\n",
    "\n",
    "6.1 Patrones de resumen.\n",
    "\n",
    "6.1.3. Contando con contadores Versión 3. Declaramos los contadores fuera de la clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing counterV3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile counterV3.py\n",
    "#!/usr/bin/env python\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "contadores={\"Netherlands\":0,\"France\":0,\"Australia\":0} \n",
    "lista=[]\n",
    "class counterV3(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        linea=line.split(\";\")\n",
    "        # Estudiamos cada token de la línea recogido#\n",
    "        for token in linea:\n",
    "            # Si el token está en el diccionario\n",
    "            if token in contadores:\n",
    "                # Contamos con el contador definido en diccionario\n",
    "                contadores[token]=contadores[token]+1\n",
    "    def mapper_final(self):\n",
    "        yield \"Bloque: \",contadores\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    counterV3.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/counterV3.root.20201214.185215.458061\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/counterV3.root.20201214.185215.458061/output\n",
      "Streaming final output from /tmp/counterV3.root.20201214.185215.458061/output...\n",
      "\"Bloque: \"\t{\"Netherlands\": 1848, \"France\": 6406, \"Australia\": 1214}\n",
      "\"Bloque: \"\t{\"Netherlands\": 2371, \"France\": 8557, \"Australia\": 1259}\n",
      "\"Bloque: \"\t{\"Netherlands\": 1255, \"France\": 4028, \"Australia\": 867}\n",
      "\"Bloque: \"\t{\"Netherlands\": 561, \"France\": 2018, \"Australia\": 363}\n",
      "Removing temp directory /tmp/counterV3.root.20201214.185215.458061...\n"
     ]
    }
   ],
   "source": [
    "!python counterV3.py archivos_datos/Online_Retail.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EJECUCIÓN EN EL CLUSTER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/lib/hadoop/bin...\n",
      "Found hadoop binary: /usr/lib/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/counterV3.root.20201214.185246.363579\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/counterV3.root.20201214.185246.363579/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/counterV3.root.20201214.185246.363579/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob6542636647194230751.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.5:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.5:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1607966861739_0011\n",
      "  Submitted application application_1607966861739_0011\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1607966861739_0011/\n",
      "  Running job: job_1607966861739_0011\n",
      "  Job job_1607966861739_0011 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1607966861739_0011 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/counterV3.root.20201214.185246.363579/output\n",
      "Counters: 31\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=45542142\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=134\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=295670\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=45542352\n",
      "\t\tHDFS: Number of bytes written=134\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=42582016\n",
      "\t\tTotal time spent by all map tasks (ms)=41584\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=41584\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=41584\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=6120\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=652\n",
      "\t\tInput split bytes=210\n",
      "\t\tMap input records=541910\n",
      "\t\tMap output records=2\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=408780800\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=315097088\n",
      "\t\tVirtual memory (bytes) snapshot=5266022400\n",
      "job output is in hdfs:///user/root/tmp/mrjob/counterV3.root.20201214.185246.363579/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/counterV3.root.20201214.185246.363579/output...\n",
      "\"Bloque: \"\t{\"Netherlands\": 1255, \"France\": 4028, \"Australia\": 867}\n",
      "\"Bloque: \"\t{\"Netherlands\": 1116, \"France\": 4529, \"Australia\": 392}\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/counterV3.root.20201214.185246.363579...\n",
      "Removing temp directory /tmp/counterV3.root.20201214.185246.363579...\n"
     ]
    }
   ],
   "source": [
    "! python counterV3.py hdfs:///archivos_datos/Online_Retail.csv -r hadoop --python-bin /opt/anaconda/bin/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
