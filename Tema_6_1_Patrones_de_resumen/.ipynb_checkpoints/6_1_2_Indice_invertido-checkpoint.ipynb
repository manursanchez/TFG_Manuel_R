{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capítulo 6\n",
    "\n",
    "6.1 Patrones de resumen.\n",
    "\n",
    "6.1.2. Índice invertido\n",
    "\n",
    "Patrón que mapea un documento con varias líneas y devuelve la palabra junto a las líneas donde aparece y el número de veces que aparece en cada línea. La clave es la palabra y el valor es una lista con la linea y veces que aparece en cada línea.\n",
    "Puede ser extrapolable a documentos siempre que estén numerados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing indexinverted.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile indexinverted.py\n",
    "#!/usr/bin/env python\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class indexinverted(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        # Cuidado con el separador\n",
    "        linea=line.split(' ')\n",
    "        #Recogemos el primer elemento de la lista \"linea\", que en este caso identifica el número de línea\n",
    "        nLinea=linea[0]\n",
    "        #Recogemos todos los elementos de la lista \"linea\" a partir de la posición 1 \n",
    "        palabras=linea[1:]\n",
    "        #para cada elemento(palabra) de la lista \"valor\" lo pasamos al Reducer en minúscula y con su número de línea. \n",
    "        for palabra in palabras:\n",
    "            yield palabra.lower(),nLinea\n",
    "    \n",
    "    #La característica especial del reducer, es que se va a encargar de hacer un tratamiento por cada palabra que encuentre\n",
    "    #Lo que vamos a controlar es el número de línea y veces que aparece una palabra en una línea, \n",
    "    #mediante una lista. El resto se encarga el Reducer. \n",
    "    def reducer(self, key, values):\n",
    "        listaValores=[]\n",
    "        listaDef=[]\n",
    "        #Introducimos en una lista los valores que vienen del mapper\n",
    "        for valor in values:\n",
    "            listaValores.append(valor)\n",
    "        \n",
    "        #Vamos a ir viendo en cada elemento de la lista que es lo que contiene,\n",
    "        #para ello usamos el contador \"posicion\" que nos permitirá recorrer y extraer\n",
    "        #cada elemento de la lista mediante su número de posición. \n",
    "        \n",
    "        for posicion in range(len(listaValores)):\n",
    "            # la subLista contendrá el número de línea/documento donde aparece la palabra y \n",
    "            # el número de veces que aparece.\n",
    "            subLista=[]\n",
    "            \n",
    "            #Metemos el número de línea/documento en la variable elementoLista\n",
    "            elementoLista=listaValores[posicion]\n",
    "            \n",
    "            #Contamos el número de elementos iguales que hay en listaValores\n",
    "            #Metemos el número de elementos en la variable numérica cuentaElementos\n",
    "            cuentaElementos=listaValores.count(elementoLista) \n",
    "            \n",
    "            #Con las dos variables anteriores construimos el elemento de la sublista\n",
    "            subLista=[listaValores[posicion],cuentaElementos]\n",
    "            \n",
    "            #Si esa sublista creada está en la lista definitiva, no la agregamos por que ya ha sido agregada anteriormente\n",
    "            #si no, lo agregamos a la lista definitiva.\n",
    "            if subLista not in listaDef:\n",
    "                listaDef.append(subLista)\n",
    "                \n",
    "        #Mandamos a la salida la palabra (key) y las líneas/documentos(listaDef), y número\n",
    "        #de veces que aparecen\n",
    "        yield key, listaDef\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    indexinverted.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/indexinverted.root.20201214.183816.522876\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/indexinverted.root.20201214.183816.522876/output\n",
      "Streaming final output from /tmp/indexinverted.root.20201214.183816.522876/output...\n",
      "\"es\"\t[[\"linea4\", 1]]\n",
      "\"mismo\"\t[[\"linea4\", 1]]\n",
      "\"nada\"\t[[\"linea3\", 1]]\n",
      "\"perro\"\t[[\"linea1\", 1], [\"linea2\", 1]]\n",
      "\"que\"\t[[\"linea2\", 1]]\n",
      "\"rapidamente\"\t[[\"linea3\", 1]]\n",
      "\"rapido\"\t[[\"linea1\", 1]]\n",
      "\"y\"\t[[\"linea3\", 1]]\n",
      "\"de\"\t[[\"linea4\", 1]]\n",
      "\"el\"\t[[\"linea1\", 1], [\"linea2\", 2], [\"linea3\", 1], [\"linea4\", 3]]\n",
      "\"camino\"\t[[\"linea4\", 1]]\n",
      "\"chavea\"\t[[\"linea2\", 1], [\"linea3\", 1]]\n",
      "\"chico\"\t[[\"linea4\", 1]]\n",
      "\"con\"\t[[\"linea2\", 1]]\n",
      "\"corre\"\t[[\"linea1\", 1], [\"linea2\", 2], [\"linea3\", 1]]\n",
      "Removing temp directory /tmp/indexinverted.root.20201214.183816.522876...\n"
     ]
    }
   ],
   "source": [
    "! python indexinverted.py archivos_datos/docejemplo.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EJECUCIÓN EN EL CLUSTER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/lib/hadoop/bin...\n",
      "Found hadoop binary: /usr/lib/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/indexinverted.root.20201214.183954.387404\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/indexinverted.root.20201214.183954.387404/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/indexinverted.root.20201214.183954.387404/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob7934080213853834604.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.5:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.5:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1607966861739_0008\n",
      "  Submitted application application_1607966861739_0008\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1607966861739_0008/\n",
      "  Running job: job_1607966861739_0008\n",
      "  Job job_1607966861739_0008 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1607966861739_0008 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/indexinverted.root.20201214.183954.387404/output\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=237\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=456\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=474\n",
      "\t\tFILE: Number of bytes written=446006\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=441\n",
      "\t\tHDFS: Number of bytes written=456\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=37204992\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=10028032\n",
      "\t\tTotal time spent by all map tasks (ms)=36333\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=36333\n",
      "\t\tTotal time spent by all reduce tasks (ms)=9793\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=9793\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=36333\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=9793\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=5290\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=1005\n",
      "\t\tInput split bytes=204\n",
      "\t\tMap input records=4\n",
      "\t\tMap output bytes=416\n",
      "\t\tMap output materialized bytes=480\n",
      "\t\tMap output records=26\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=760623104\n",
      "\t\tReduce input groups=15\n",
      "\t\tReduce input records=26\n",
      "\t\tReduce output records=15\n",
      "\t\tReduce shuffle bytes=480\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=52\n",
      "\t\tTotal committed heap usage (bytes)=657457152\n",
      "\t\tVirtual memory (bytes) snapshot=7888523264\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/indexinverted.root.20201214.183954.387404/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/indexinverted.root.20201214.183954.387404/output...\n",
      "\"camino\"\t[[\"linea4\", 1]]\n",
      "\"chavea\"\t[[\"linea2\", 1], [\"linea3\", 1]]\n",
      "\"chico\"\t[[\"linea4\", 1]]\n",
      "\"con\"\t[[\"linea2\", 1]]\n",
      "\"corre\"\t[[\"linea3\", 1], [\"linea2\", 2], [\"linea1\", 1]]\n",
      "\"de\"\t[[\"linea4\", 1]]\n",
      "\"el\"\t[[\"linea4\", 3], [\"linea2\", 2], [\"linea3\", 1], [\"linea1\", 1]]\n",
      "\"es\"\t[[\"linea4\", 1]]\n",
      "\"mismo\"\t[[\"linea4\", 1]]\n",
      "\"nada\"\t[[\"linea3\", 1]]\n",
      "\"perro\"\t[[\"linea1\", 1], [\"linea2\", 1]]\n",
      "\"que\"\t[[\"linea2\", 1]]\n",
      "\"rapidamente\"\t[[\"linea3\", 1]]\n",
      "\"rapido\"\t[[\"linea1\", 1]]\n",
      "\"y\"\t[[\"linea3\", 1]]\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/indexinverted.root.20201214.183954.387404...\n",
      "Removing temp directory /tmp/indexinverted.root.20201214.183954.387404...\n"
     ]
    }
   ],
   "source": [
    "! python indexinverted.py hdfs:///archivos_datos/docejemplo.txt -r hadoop --python-bin /opt/anaconda/bin/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
