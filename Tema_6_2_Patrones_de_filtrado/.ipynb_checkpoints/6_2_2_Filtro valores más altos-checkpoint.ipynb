{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capítulo 6\n",
    "\n",
    "6.2 Patrones de filtrado.\n",
    "\n",
    "6.2.2 Filtro valores más altos\n",
    "\n",
    "Ejemplo de patrón de filtrado para extraer los N valores más altos.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing topN.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile topN.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "\n",
    "TopN=10 #Los N valores más altos que vamos a extraer\n",
    "\n",
    "class topN(MRJob):\n",
    "    \n",
    "    #Método que extrae los N valores más altos de una lista de valores.\n",
    "    def extraeTopN(self,listaValores):\n",
    "        valoresMasAltos = []\n",
    "        cuenta=0\n",
    "        #Comprobamos que el número de valores Top que necesitamos no sea mayor que el tamaño de la lista\n",
    "        if TopN>len(listaValores):#si lo es, N lo limitamos al tamaño máximo de la lista.\n",
    "            nValores=len(listaValores)\n",
    "        else:\n",
    "            nValores=TopN\n",
    "        \n",
    "        #Buscamos los N valores distintos más altos. Empezamos por el final de la lista\n",
    "        #que al estar ordenada, los valores más altos están al final. Pero puede haber valores repetidos\n",
    "        #y eso es lo que vamos a hacer con estas líneas de código, evitar que se repitan valores.\n",
    "        \n",
    "        \n",
    "        posicion=len(listaValores)-1 #Almacenamos la última posición de la lista\n",
    "        \n",
    "        while cuenta<nValores: #Como vamos a coger los N valores, \"cuenta\" la limitamos al número de valores \n",
    "                                #que quiera localizar\n",
    "            try:\n",
    "                if listaValores[posicion] not in valoresMasAltos: #Si el valor no está en los valores más altos, lo agregamos\n",
    "                    valoresMasAltos.append(listaValores[posicion])\n",
    "                    cuenta+=1 # si hemos agregado un valor aumentamos cuenta \n",
    "                    \n",
    "                posicion-=1 # tanto si hemos agregado un valor como si no, disminuimos la posición \n",
    "                        # de la lista para buscar un nuevo valor. Puede darse el caso que posición llegue a -1\n",
    "                        #(cuenta y posición no van al unísono, sobre todo cuando hay valores repetidos)\n",
    "                        #esto genera una excepción de índice fuera de rango. Es recogida por el try-catch y devuelve\n",
    "                        #los valores correctos por que ha terminado.\n",
    "            except:\n",
    "                break\n",
    "                \n",
    "        return valoresMasAltos\n",
    "    \n",
    "    #Inicializamos la estructura que nos permitirá contener los valores que nos llegan del Map\n",
    "    def mapper_init(self):\n",
    "        self.topNmap = []\n",
    "    \n",
    "    def mapper(self,_, line):\n",
    "        self.registro=line.rstrip('/n').split(';') \n",
    "        encontrado=re.search('[a-zA-Z]',self.registro[5])#En este caso el valor lo tengo en la posción 5 de la línea\n",
    "        #Preguntamos si ha encontrado la expresion regular. Si no la ha encontrado, procesamos dato\n",
    "        if encontrado==None:\n",
    "            self.topNmap.append(float(self.registro[5]))\n",
    "        \n",
    "    def mapper_final(self):\n",
    "        self.topNmap.sort() #se ordena la lista\n",
    "        #La función extraeTopN extrae los 10 elementos últimos de la lista, que se envían a reducer\n",
    "        yield \"TopNmap:\", self.extraeTopN(self.topNmap)\n",
    "        \n",
    " #===============================REDUCER=======================================     \n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "        listaValores=[]\n",
    "        \n",
    "        # metemos los valores en una lista\n",
    "        for valores in values:\n",
    "            listaValores.extend(valores)\n",
    "        \n",
    "        # ordenamos la lista    \n",
    "        listaValores.sort()\n",
    "        yield \"TopNreducer:\", self.extraeTopN(listaValores)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    topN.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/topN.root.20201214.191225.462453\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/topN.root.20201214.191225.462453/output\n",
      "Streaming final output from /tmp/topN.root.20201214.191225.462453/output...\n",
      "\"TopNreducer:\"\t[38970.0, 17836.46, 16888.02, 16453.71, 13541.33, 13474.79, 11586.5, 11062.06, 8286.22, 8142.75]\n",
      "Removing temp directory /tmp/topN.root.20201214.191225.462453...\n"
     ]
    }
   ],
   "source": [
    "!python topN.py archivos_datos/ventas_UK.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EJECUCIÓN EN EL CLUSTER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/lib/hadoop/bin...\n",
      "Found hadoop binary: /usr/lib/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/topN.root.20201214.191338.134298\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/topN.root.20201214.191338.134298/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/topN.root.20201214.191338.134298/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob7243219805135454558.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.5:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.5:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1607966861739_0014\n",
      "  Submitted application application_1607966861739_0014\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1607966861739_0014/\n",
      "  Running job: job_1607966861739_0014\n",
      "  Job job_1607966861739_0014 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1607966861739_0014 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/topN.root.20201214.191338.134298/output\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=41916330\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=112\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=219\n",
      "\t\tFILE: Number of bytes written=445115\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=41916532\n",
      "\t\tHDFS: Number of bytes written=112\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=1\n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=48480256\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=12735488\n",
      "\t\tTotal time spent by all map tasks (ms)=47344\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=47344\n",
      "\t\tTotal time spent by all reduce tasks (ms)=12437\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=12437\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=47344\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=12437\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=7770\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=941\n",
      "\t\tInput split bytes=202\n",
      "\t\tMap input records=495479\n",
      "\t\tMap output bytes=209\n",
      "\t\tMap output materialized bytes=225\n",
      "\t\tMap output records=2\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=740560896\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=2\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=225\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=4\n",
      "\t\tTotal committed heap usage (bytes)=651165696\n",
      "\t\tVirtual memory (bytes) snapshot=7887515648\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/topN.root.20201214.191338.134298/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/topN.root.20201214.191338.134298/output...\n",
      "\"TopNreducer:\"\t[38970.0, 17836.46, 16888.02, 16453.71, 13541.33, 13474.79, 11586.5, 11062.06, 8286.22, 8142.75]\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/topN.root.20201214.191338.134298...\n",
      "Removing temp directory /tmp/topN.root.20201214.191338.134298...\n"
     ]
    }
   ],
   "source": [
    "! python topN.py hdfs:///archivos_datos/ventas_UK.csv -r hadoop --python-bin /opt/anaconda/bin/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
